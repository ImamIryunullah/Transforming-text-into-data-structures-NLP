{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p3Xb__aEMJS"
      },
      "source": [
        "# One Hot Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pcVNjW0GEMJU",
        "outputId": "7c9e957a-2566-4649-b8dc-61efa9c838b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\IMAM\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\IMAM\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RohBDkPgEMJW"
      },
      "source": [
        "## We take only 1 sentence as input here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZIuC2_olEMJX"
      },
      "outputs": [],
      "source": [
        "sentence = [\"We are reading about Natural Language Processing Here\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yx6AEIdWEMJY",
        "outputId": "adba9ae8-50fc-477c-b7a2-5e7fdc6c7b8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    We are reading about Natural Language Processi...\n",
              "dtype: object"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus = pd.Series(sentence)\n",
        "corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wd9oNnHPEMJY"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TlvXbFJgEMJZ"
      },
      "outputs": [],
      "source": [
        "def text_clean(corpus, keep_list):\n",
        "    '''\n",
        "    Purpose : Function to keep only alphabets, digits and certain words (punctuations, qmarks, tabs etc. removed)\n",
        "\n",
        "    Input : Takes a text corpus, 'corpus' to be cleaned along with a list of words, 'keep_list', which have to be retained\n",
        "            even after the cleaning process\n",
        "\n",
        "    Output : Returns the cleaned text corpus\n",
        "\n",
        "    '''\n",
        "    cleaned_list = []\n",
        "    for row in corpus:\n",
        "        qs = []\n",
        "        for word in row.split():\n",
        "            if word not in keep_list:\n",
        "                p1 = re.sub(pattern='[^a-zA-Z0-9]',repl=' ',string=word)\n",
        "                p1 = p1.lower()\n",
        "                qs.append(p1)\n",
        "            else : qs.append(word)\n",
        "        cleaned_list.append(' '.join(qs))\n",
        "    return pd.Series(cleaned_list, dtype=\"string\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uzGvi4TDEMJa"
      },
      "outputs": [],
      "source": [
        "def lemmatize(corpus):\n",
        "    lem = WordNetLemmatizer()\n",
        "    corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "y2qyAIpfEMJa"
      },
      "outputs": [],
      "source": [
        "def stem(corpus, stem_type = None):\n",
        "    if stem_type == 'snowball':\n",
        "        stemmer = SnowballStemmer(language = 'english')\n",
        "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    else :\n",
        "        stemmer = PorterStemmer()\n",
        "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UFYJ9BGMEMJb"
      },
      "outputs": [],
      "source": [
        "def stopwords_removal(corpus):\n",
        "    wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
        "    stop = set(stopwords.words('english'))\n",
        "    for word in wh_words:\n",
        "        stop.remove(word)\n",
        "    corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iJZjDSgnEMJc"
      },
      "outputs": [],
      "source": [
        "def preprocess(corpus, keep_list, cleaning = True, stemming = False, stem_type = None, lemmatization = False, remove_stopwords = True):\n",
        "    '''\n",
        "    Purpose : Function to perform all pre-processing tasks (cleaning, stemming, lemmatization, stopwords removal etc.)\n",
        "\n",
        "    Input :\n",
        "    'corpus' - Text corpus on which pre-processing tasks will be performed\n",
        "    'keep_list' - List of words to be retained during cleaning process\n",
        "    'cleaning', 'stemming', 'lemmatization', 'remove_stopwords' - Boolean variables indicating whether a particular task should\n",
        "                                                                  be performed or not\n",
        "    'stem_type' - Choose between Porter stemmer or Snowball(Porter2) stemmer. Default is \"None\", which corresponds to Porter\n",
        "                  Stemmer. 'snowball' corresponds to Snowball Stemmer\n",
        "\n",
        "    Note : Either stemming or lemmatization should be used. There's no benefit of using both of them together\n",
        "\n",
        "    Output : Returns the processed text corpus\n",
        "\n",
        "    '''\n",
        "\n",
        "    if cleaning == True:\n",
        "        corpus = text_clean(corpus, keep_list)\n",
        "\n",
        "    if remove_stopwords == True:\n",
        "        corpus = stopwords_removal(corpus)\n",
        "    else :\n",
        "        corpus = [[x for x in x.split()] for x in corpus]\n",
        "\n",
        "    if lemmatization == True:\n",
        "        corpus = lemmatize(corpus)\n",
        "\n",
        "\n",
        "    if stemming == True:\n",
        "        corpus = stem(corpus, stem_type)\n",
        "\n",
        "    corpus = [' '.join(x) for x in corpus]\n",
        "\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "k0zI4SXBEMJc",
        "outputId": "e2cfef3c-b31b-4d34-f62f-84ca66bd3b86"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['read natural language process']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Preprocessing with Lemmatization here\n",
        "preprocessed_corpus = preprocess(corpus, keep_list = [], stemming = False, stem_type = None,\n",
        "                                lemmatization = True, remove_stopwords = True)\n",
        "preprocessed_corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgXNmp2jEMJc"
      },
      "source": [
        "## Building the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kFWDcAEvEMJd",
        "outputId": "e92585fd-5d72-40ad-c05f-f49a06cfb726"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['read', 'process', 'natural', 'language']\n"
          ]
        }
      ],
      "source": [
        "set_of_words = set()\n",
        "for word in preprocessed_corpus[0].split():\n",
        "    set_of_words.add(word)\n",
        "vocab = list(set_of_words)\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXn5REyqEMJd"
      },
      "source": [
        "## Fetching the position of each word in the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Ln-LkUTAEMJd",
        "outputId": "130e583a-27b0-402b-f85d-83db67521a8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'read': 0, 'process': 1, 'natural': 2, 'language': 3}\n"
          ]
        }
      ],
      "source": [
        "position = {}\n",
        "for i, token in enumerate(vocab):\n",
        "    position[token] = i\n",
        "print(position)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSMfwIJYEMJe"
      },
      "source": [
        "## Instantiating the one hot matrix\n",
        "\n",
        "### Note here every row in the matrix corresponds to the One Hot vector for an individual term"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "6IdJOFzVEMJe",
        "outputId": "ac4dd8d6-a162-4973-afbc-c25e9ecb0c2e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4, 4)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "one_hot_matrix = np.zeros((len(preprocessed_corpus[0].split()), len(vocab)))\n",
        "one_hot_matrix.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ineBDDYsEMJf"
      },
      "source": [
        "## Building One Hot Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qy45Zp0oEMJf"
      },
      "outputs": [],
      "source": [
        "for i, token in enumerate(preprocessed_corpus[0].split()):\n",
        "    one_hot_matrix[i][position[token]] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLX__37aEMJf"
      },
      "source": [
        "## Visualizing the One Hot Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "IpYsnuorEMJf",
        "outputId": "ab957a5b-0271-4b69-e86b-40f44c4ffbe5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0.],\n",
              "       [0., 0., 1., 0.],\n",
              "       [0., 0., 0., 1.],\n",
              "       [0., 1., 0., 0.]])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "one_hot_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh7FF2XfEMJf"
      },
      "source": [
        "## Inference\n",
        "\n",
        "The first row corresponds to the One Hot vector of *read*,\n",
        "\n",
        "second for *natural*,\n",
        "\n",
        "third for *language* and\n",
        "\n",
        "the final one for *process*\n",
        "\n",
        "based on their respective indices in the vocabulary"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
