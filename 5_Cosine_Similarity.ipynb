{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMTVuafuEDEJ"
      },
      "source": [
        "# Measuring Cosine Similarity between Document Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ucSTAPf1EDEK",
        "outputId": "25532522-8202-4459-cc0c-2073da4bb031"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\IMAM\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\IMAM\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "479fxBBAEDEL"
      },
      "source": [
        "## Building a corpus of sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TorjofDFEDEL"
      },
      "outputs": [],
      "source": [
        "sentences = [\"We are reading about Natural Language Processing Here\",\n",
        "            \"Natural Language Processing making computers comprehend language data\",\n",
        "            \"The field of Natural Language Processing is evolving everyday\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xxicpFqeEDEL",
        "outputId": "4a44554d-786b-4111-820e-f8e0693208f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    We are reading about Natural Language Processi...\n",
              "1    Natural Language Processing making computers c...\n",
              "2    The field of Natural Language Processing is ev...\n",
              "dtype: object"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus = pd.Series(sentences)\n",
        "corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs9MA9VZEDEM"
      },
      "source": [
        "## Data preprocessing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fYm7gwVAEDEM"
      },
      "outputs": [],
      "source": [
        "def text_clean(corpus, keep_list):\n",
        "    '''\n",
        "    Purpose : Function to keep only alphabets, digits and certain words (punctuations, qmarks, tabs etc. removed)\n",
        "\n",
        "    Input : Takes a text corpus, 'corpus' to be cleaned along with a list of words, 'keep_list', which have to be retained\n",
        "            even after the cleaning process\n",
        "\n",
        "    Output : Returns the cleaned text corpus\n",
        "\n",
        "    '''\n",
        "    cleaned_list = []\n",
        "    for row in corpus:\n",
        "        qs = []\n",
        "        for word in row.split():\n",
        "            if word not in keep_list:\n",
        "                p1 = re.sub(pattern='[^a-zA-Z0-9]',repl=' ',string=word)\n",
        "                p1 = p1.lower()\n",
        "                qs.append(p1)\n",
        "            else : qs.append(word)\n",
        "        cleaned_list.append(' '.join(qs))\n",
        "    return pd.Series(cleaned_list, dtype=\"string\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WrhXms9-EDEM"
      },
      "outputs": [],
      "source": [
        "def lemmatize(corpus):\n",
        "    lem = WordNetLemmatizer()\n",
        "    corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hJkvxu_yEDEN"
      },
      "outputs": [],
      "source": [
        "def stem(corpus, stem_type = None):\n",
        "    if stem_type == 'snowball':\n",
        "        stemmer = SnowballStemmer(language = 'english')\n",
        "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    else :\n",
        "        stemmer = PorterStemmer()\n",
        "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "O6DK8KNJEDEN"
      },
      "outputs": [],
      "source": [
        "def stopwords_removal(corpus):\n",
        "    wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
        "    stop = set(stopwords.words('english'))\n",
        "    for word in wh_words:\n",
        "        stop.remove(word)\n",
        "    corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Sy2prApXEDEN"
      },
      "outputs": [],
      "source": [
        "def preprocess(corpus, keep_list, cleaning = True, stemming = False, stem_type = None, lemmatization = False, remove_stopwords = True):\n",
        "    '''\n",
        "    Purpose : Function to perform all pre-processing tasks (cleaning, stemming, lemmatization, stopwords removal etc.)\n",
        "\n",
        "    Input :\n",
        "    'corpus' - Text corpus on which pre-processing tasks will be performed\n",
        "    'keep_list' - List of words to be retained during cleaning process\n",
        "    'cleaning', 'stemming', 'lemmatization', 'remove_stopwords' - Boolean variables indicating whether a particular task should\n",
        "                                                                  be performed or not\n",
        "    'stem_type' - Choose between Porter stemmer or Snowball(Porter2) stemmer. Default is \"None\", which corresponds to Porter\n",
        "                  Stemmer. 'snowball' corresponds to Snowball Stemmer\n",
        "\n",
        "    Note : Either stemming or lemmatization should be used. There's no benefit of using both of them together\n",
        "\n",
        "    Output : Returns the processed text corpus\n",
        "\n",
        "    '''\n",
        "\n",
        "    if cleaning == True:\n",
        "        corpus = text_clean(corpus, keep_list)\n",
        "\n",
        "    if remove_stopwords == True:\n",
        "        corpus = stopwords_removal(corpus)\n",
        "    else :\n",
        "        corpus = [[x for x in x.split()] for x in corpus]\n",
        "\n",
        "    if lemmatization == True:\n",
        "        corpus = lemmatize(corpus)\n",
        "\n",
        "\n",
        "    if stemming == True:\n",
        "        corpus = stem(corpus, stem_type)\n",
        "\n",
        "    corpus = [' '.join(x) for x in corpus]\n",
        "\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7ST6gRhPEDEN",
        "outputId": "2856125d-f840-45fe-ee24-3d6bdd894afa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['read natural language process',\n",
              " 'natural language process make computers comprehend language data',\n",
              " 'field natural language process evolve everyday']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Preprocessing with Lemmatization here\n",
        "preprocessed_corpus = preprocess(corpus, keep_list = [], stemming = False, stem_type = None,\n",
        "                                lemmatization = True, remove_stopwords = True)\n",
        "preprocessed_corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW-djQOVEDEO"
      },
      "source": [
        "## Cosine Similarity Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9VQrrtmTEDEO"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(vector1, vector2):\n",
        "    vector1 = np.array(vector1)\n",
        "    vector2 = np.array(vector2)\n",
        "    return np.dot(vector1, vector2) / (np.sqrt(np.sum(vector1**2)) * np.sqrt(np.sum(vector2**2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkMllflMEDEO"
      },
      "source": [
        "## CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hWari6quEDEO"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(preprocessed_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6l34osTpEDEO",
        "outputId": "5e4e9cf9-3950-49c8-9b39-f57fc9ddb1d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['comprehend' 'computers' 'data' 'everyday' 'evolve' 'field' 'language'\n",
            " 'make' 'natural' 'process' 'read']\n",
            "[[0 0 0 0 0 0 1 0 1 1 1]\n",
            " [1 1 1 0 0 0 2 1 1 1 0]\n",
            " [0 0 0 1 1 1 1 0 1 1 0]]\n"
          ]
        }
      ],
      "source": [
        "print(vectorizer.get_feature_names_out())\n",
        "print(bow_matrix.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcqiYlXcEDEO"
      },
      "source": [
        "## Cosine similarity between the document vectors built using CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "w0zNhutNEDEP",
        "outputId": "2ec53b7f-1b86-4686-a53a-48974f39b98a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The cosine similarity between the documents  0 and 1 is:  0.6324555320336759\n",
            "The cosine similarity between the documents  0 and 2 is:  0.6123724356957946\n",
            "The cosine similarity between the documents  1 and 2 is:  0.5163977794943223\n"
          ]
        }
      ],
      "source": [
        "for i in range(bow_matrix.shape[0]):\n",
        "    for j in range(i + 1, bow_matrix.shape[0]):\n",
        "        print(\"The cosine similarity between the documents \", i, \"and\", j, \"is: \",\n",
        "              cosine_similarity(bow_matrix.toarray()[i], bow_matrix.toarray()[j]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO4b5zUAEDEP"
      },
      "source": [
        "## TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1oBlYhCBEDEP"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "tf_idf_matrix = vectorizer.fit_transform(preprocessed_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pDpyaxleEDEP",
        "outputId": "8eface79-a4f8-4bcd-e290-d2cba4874faf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['comprehend' 'computers' 'data' 'everyday' 'evolve' 'field' 'language'\n",
            " 'make' 'natural' 'process' 'read']\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.41285857 0.         0.41285857 0.41285857 0.69903033]\n",
            " [0.40512186 0.40512186 0.40512186 0.         0.         0.\n",
            "  0.478543   0.40512186 0.2392715  0.2392715  0.        ]\n",
            " [0.         0.         0.         0.49711994 0.49711994 0.49711994\n",
            "  0.29360705 0.         0.29360705 0.29360705 0.        ]]\n",
            "\n",
            "The shape of the TF-IDF matrix is:  (3, 11)\n"
          ]
        }
      ],
      "source": [
        "print(vectorizer.get_feature_names_out())\n",
        "print(tf_idf_matrix.toarray())\n",
        "print(\"\\nThe shape of the TF-IDF matrix is: \", tf_idf_matrix.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcJj8UK4EDEP"
      },
      "source": [
        "## Cosine similarity between the document vectors built using TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Knd20OQZEDEP",
        "outputId": "72abfe8f-7b4b-4c8e-8132-edfe57865cca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The cosine similarity between the documents  0 and 1 is:  0.39514115766749125\n",
            "The cosine similarity between the documents  0 and 2 is:  0.36365455673761865\n",
            "The cosine similarity between the documents  1 and 2 is:  0.2810071916500233\n"
          ]
        }
      ],
      "source": [
        "for i in range(tf_idf_matrix.shape[0]):\n",
        "    for j in range(i + 1, tf_idf_matrix.shape[0]):\n",
        "        print(\"The cosine similarity between the documents \", i, \"and\", j, \"is: \",\n",
        "              cosine_similarity(tf_idf_matrix.toarray()[i], tf_idf_matrix.toarray()[j]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
